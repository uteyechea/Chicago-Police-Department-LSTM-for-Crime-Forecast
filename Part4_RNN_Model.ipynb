{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part4_RNN_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uteyechea/crime-prediction-using-artificial-intelligence/blob/master/Part4_RNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rsvlBQYCrE4I"
      },
      "source": [
        "# Part 4 Recurrent Neural Net model\n",
        "Define, train and generate crime predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1g6frEW536I",
        "colab_type": "text"
      },
      "source": [
        "##4.1 Dependencies, mount Google Drive and set system path\n",
        "Import the relevant packages we will use to train the RNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riVZCVK65QTH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "2fb957c9-5dc6-4799-f6b2-7ffd9d0934ac"
      },
      "source": [
        "# Import Tensorflow 2.0\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf \n",
        "\n",
        "#Mount my Google Drive \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#Define global path to working directory\n",
        "path='/content/drive/My Drive/Colab Notebooks/crime_prediction'\n",
        "\n",
        "#Update our path to import from \n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/crime_prediction/libs/')\n",
        "import util\n",
        "\n",
        "# Import all remaining packages\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "import regex as re\n",
        "import random\n",
        "\n",
        "# Check that we are using a GPU, if not switch runtime\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiS-9qDy8f-x",
        "colab_type": "text"
      },
      "source": [
        "##4.2 Import training file data\n",
        "\n",
        "Load file contents to memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7dFnP5q3Jve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "1d9ddd98-dfe6-4bc1-82fb-53aca6fca553"
      },
      "source": [
        "def load_training_data(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        text = f.read()\n",
        "    windows = extract_windows(text)\n",
        "    return windows\n",
        "\n",
        "def extract_windows(text):\n",
        "    pattern = '\\n\\n(.*?)\\n\\n'\n",
        "    search_results = re.findall(pattern, text, overlapped=True, flags=re.DOTALL)\n",
        "    windows = [window for window in search_results]\n",
        "    print(\"Found {} autocorrelated windows in text\".format(len(windows)))\n",
        "    return windows\n",
        "\n",
        "file_path=os.path.join(path,'data','training','theft.csv')\n",
        "windows=load_training_data(file_path)\n",
        "\n",
        "# Print one of the songs to inspect it in greater detail!\n",
        "random_window=random.randint(0,len(windows))\n",
        "example_window = windows[random_window]\n",
        "print(\"Example: window {}\".format(random_window))\n",
        "print(example_window)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 35 autocorrelated windows in text\n",
            "Example: window 2\n",
            "0.107\n",
            "0.393\n",
            "0.250\n",
            "0.357\n",
            "0.429\n",
            "0.393\n",
            "0.429\n",
            "0.250\n",
            "0.536\n",
            "0.429\n",
            "0.393\n",
            "0.286\n",
            "0.571\n",
            "0.179\n",
            "0.286\n",
            "0.393\n",
            "0.286\n",
            "0.286\n",
            "0.214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbjOw8jD8egI",
        "colab_type": "text"
      },
      "source": [
        "Join all data in a single data memory location, then create a vocabulary of all unique symbols in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d5139521-d5a4-412f-ba7b-ee77e5b09e5f"
      },
      "source": [
        "# Join our list of windows into a single data string containing all windows\n",
        "windows_joined='\\n\\n'.join(windows)\n",
        "#print(windows_joined)\n",
        "# Find all unique characters in the joined data string\n",
        "vocabulary = sorted(set(windows_joined))\n",
        "print('There are {} unique characters in the dataset'.format(len(vocabulary)))\n",
        "print(vocabulary)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 12 unique characters in the dataset\n",
            "['\\n', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## 4.3 Process the dataset for the learning task\n",
        "\n",
        "Let's take a step back and consider our prediction task. We're trying to train a RNN model to learn patterns in crime, and then use this model to generate (i.e., predict) the expected crime locations and times.\n",
        "\n",
        "The prediction task is all about given a character, or a sequence of characters, what is the most probable next character? \n",
        "\n",
        "Training the RNN consiste of feeding it an input made of a sequence of characters, so that the model learns to predict the output, i.e. the most likely character at each time step. \n",
        "\n",
        "RNNs maintain an internal state that remember all characters seen up until a given moment thus all history will be taken into account in generating a prediction sequence or character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### 4.3.1 Vectorize the text\n",
        "\n",
        "In order to make a general AI algortithm, that is one that is able to predict, text, numbers or symbols as in music, it is common practice to map all characters in the training data to a set of integers. Why? I need to learn more to answer this question. \n",
        "https://www.tensorflow.org/tutorials/text/text_generation\n",
        "But i can say that for sure, this mapping allows the RNN to learn the appropiate data format, for example: n columns of numbers or words then a new line, wher n could be anything from a random number to a fixed number. \n",
        "\n",
        "\n",
        "\n",
        "So the usual way to proceed is as follows,\n",
        "we will we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters, like a dictionary in the common sense. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IalZLbvOzf-F",
        "colab": {}
      },
      "source": [
        "### Define numerical representation of text ###\n",
        "\n",
        "# Create a mapping from character to unique index.\n",
        "# For example, to get the index of the character \"d\", \n",
        "#   we can evaluate `char2idx[\"d\"]`.  \n",
        "char2idx = {u:i for i, u in enumerate(vocabulary)}\n",
        "\n",
        "# Create a mapping from indices to characters. This is\n",
        "#   the inverse of char2idx and allows us to convert back\n",
        "#   from unique index to the character in our vocabulary.\n",
        "idx2char = np.array(vocabulary)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "This gives us an integer representation for each character. Observe that the unique characters (i.e., our vocabulary) in the text are mapped as indices from 0 to `len(unique)`. Let's take a peek at this numerical representation of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FYyNlCNXymwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "fef8f02f-90b5-4ab1-98a6-e5285402bd4e"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(len(vocabulary))):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '.' :   1,\n",
            "  '0' :   2,\n",
            "  '1' :   3,\n",
            "  '2' :   4,\n",
            "  '3' :   5,\n",
            "  '4' :   6,\n",
            "  '5' :   7,\n",
            "  '6' :   8,\n",
            "  '7' :   9,\n",
            "  '8' :  10,\n",
            "  '9' :  11,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-LnKyu4dczc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e40b426d-e566-46d7-9966-7c8c76013ef5"
      },
      "source": [
        "### Vectorize the criminal data string ###\n",
        "def vectorize_string(string):\n",
        "  vectorized_output = np.array([char2idx[char] for char in string])\n",
        "  return vectorized_output\n",
        "\n",
        "vectorized_windows = vectorize_string(windows_joined)\n",
        "print('All autocorrelated crime data windows add up to {} integer numeric characters'.format(len(vectorized_windows)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All autocorrelated crime data windows add up to 4023 integer numeric characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqxpSuZ1w-ub",
        "colab_type": "text"
      },
      "source": [
        "We can also look at how the first part of the text is mapped to an integer representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l1VKcQHcymwb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "acb3d207-a8ea-43b7-c9a4-4cea1a743cc3"
      },
      "source": [
        "print ('{} ---- characters mapped to int ----> {}'.format(repr(windows_joined[:10]), vectorized_windows[:10]))\n",
        "# check that vectorized_songs is a numpy array\n",
        "assert isinstance(vectorized_windows, np.ndarray), \"returned result should be a numpy array\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'0.286\\n0.42' ---- characters mapped to int ----> [ 2  1  4 10  8  0  2  1  6  4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### 4.3.2 Create training examples and targets\n",
        "\n",
        "Our next step is to actually divide the text into example sequences that we'll use during training. \n",
        "\n",
        "Each input sequence that we feed into our RNN will contain `seq_length` characters from the text.\n",
        "\n",
        "We'll also need to define a target sequence for each input sequence, which will be used in training the RNN to predict the next character. \n",
        "\n",
        "For each input, the corresponding target will contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "To do this, we'll break the text into chunks of `seq_length+1`. Suppose `seq_length` is 4 and our text is \"Hello\". Then, our input sequence is \"Hell\" and the target sequence is \"ello\".\n",
        "\n",
        "The batch method will then let us convert this stream of character indices to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LF-N8F7BoDRi",
        "colab": {}
      },
      "source": [
        "### Batch definition to create training examples ###\n",
        "\n",
        "def get_batch(vectorized_windows, seq_length, batch_size):\n",
        "  # the length of the vectorized songs string\n",
        "  n = vectorized_windows.shape[0] - 1\n",
        "  # randomly choose the starting indices for the examples in the training batch\n",
        "  idx = np.random.choice(n-seq_length, batch_size)\n",
        "\n",
        "  '''TODO: construct a list of input sequences for the training batch'''\n",
        "  input_batch = [vectorized_windows[i : i+seq_length] for i in idx]\n",
        "  # input_batch = # TODO\n",
        "  '''TODO: construct a list of output sequences for the training batch'''\n",
        "  target_batch = [vectorized_windows[i+1 : i+seq_length+1] for i in idx]\n",
        "  # output_batch = # TODO\n",
        "\n",
        "  # x_batch, y_batch provide the true inputs and targets for network training\n",
        "  x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
        "  y_batch = np.reshape(target_batch, [batch_size, seq_length])\n",
        "  return x_batch, y_batch"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_33OHL3b84i0"
      },
      "source": [
        "For each of these vectors, each index is processed at a single time step. So, for the input at time step 0, the model receives the index for the first character in the sequence, and tries to predict the index of the next character. At the next timestep, it does the same thing, but the RNN considers the information from the previous step, i.e., its updated state, in addition to the current input.\n",
        "\n",
        "We can make this concrete by taking a look at how this works over the first several characters in our text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0eBu9WZG84i0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "2f0e78d7-c7c2-41cf-ca47-379c014a779b"
      },
      "source": [
        "x_batch, y_batch = get_batch(vectorized_windows, seq_length=5, batch_size=1)\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
        "    print(\"Step {:3d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step   0\n",
            "  input: 0 ('\\n')\n",
            "  expected output: 2 ('0')\n",
            "Step   1\n",
            "  input: 2 ('0')\n",
            "  expected output: 1 ('.')\n",
            "Step   2\n",
            "  input: 1 ('.')\n",
            "  expected output: 3 ('1')\n",
            "Step   3\n",
            "  input: 3 ('1')\n",
            "  expected output: 6 ('4')\n",
            "Step   4\n",
            "  input: 6 ('4')\n",
            "  expected output: 5 ('3')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## 4.4 The Recurrent Neural Network (RNN) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "The RNN model is based off the LSTM architecture, where we use a state vector to maintain information about the temporal relationships between consecutive characters. \n",
        "\n",
        "The final output of the LSTM is then fed into a fully connected [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer where we'll output a softmax over each character in the vocabulary, and then sample from this distribution to predict the next character. \n",
        "\n",
        "To define the RNN model we will use the Keras API, specifically, [`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential)\n",
        "\n",
        ". L layers are used to define the model, where L is greater than or equal to 3. The three main layers are:\n",
        "\n",
        "* [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): This is the input layer, consisting of a trainable lookup table that maps the numbers of each character to a vector with `embedding_dim` dimensions.\n",
        "* [`tf.keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Our LSTM network, with size `units=rnn_units`. \n",
        "* [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): The output layer, with `vocab_size` outputs.\n",
        "\n",
        "From here we will be upgrading the model with a stacked LSTM architecture. The number of LSTM layers will be a function of performance vs computing power required.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/uteyechea/crime-prediction-using-artificial-intelligence/master/pictures/stacked-lstm-architecture.png\" alt=\"Drawing\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlaOqndqBmJo",
        "colab_type": "text"
      },
      "source": [
        "### 4.4.1 Define the RNN type \n",
        "\n",
        "Here we will define the type of RNN. As  discussed before the type will be based on the LSTM architecture, the number of times we use this function will be equivalent to the number of LSTM layers in our RNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DsWzojvkbc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LSTM(rnn_units): \n",
        "  return tf.keras.layers.LSTM(\n",
        "    rnn_units, \n",
        "    return_sequences=True, \n",
        "    recurrent_initializer='glorot_uniform',\n",
        "    recurrent_activation='sigmoid',\n",
        "    stateful=True,\n",
        "  )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbWU4dMJmMvq",
        "colab_type": "text"
      },
      "source": [
        "From https://www.tensorflow.org/tutorials/text/text_generation we define the model using the `build_model` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "colab": {}
      },
      "source": [
        "### Defining the RNN Model ###\n",
        "\n",
        "'''TODO: Add LSTM and Dense layers to define the RNN model using the Sequential API.'''\n",
        "def build_model(vocabulary_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    # Layer 1: Embedding layer to transform indices into dense vectors \n",
        "    #   of a fixed embedding size\n",
        "    tf.keras.layers.Embedding(vocabulary_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "    # Layer 2-5: LSTM with `rnn_units` number of units. \n",
        "    LSTM(rnn_units), \n",
        "    LSTM(rnn_units), \n",
        "    LSTM(rnn_units), \n",
        "    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n",
        "    #   into the vocabulary size. \n",
        "    tf.keras.layers.Dense(vocabulary_size)\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Build a simple model with default hyperparameters. You will get the \n",
        "#   chance to change these later.\n",
        "model = build_model(len(vocabulary), embedding_dim=256, rnn_units=1024, batch_size=32)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "### 4.4.2 Review RNN model design\n",
        "\n",
        "We can use the `Model.summary` function to print out a summary of our model's internal workings. Here we can check the layers in the model, the shape of the output of each of the layers, the batch size, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwG1DD6rDrRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "634a275d-075a-42b6-84c7-635ff8ddbe30"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (32, None, 256)           3072      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (32, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (32, None, 1024)          8392704   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (32, None, 1024)          8392704   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (32, None, 12)            12300     \n",
            "=================================================================\n",
            "Total params: 22,047,756\n",
            "Trainable params: 22,047,756\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xeDn5nZD0LX",
        "colab_type": "text"
      },
      "source": [
        "We can also quickly check the dimensionality of our output, using a sequence length of 100. Note that the model can be run on inputs of any length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8279ce5f-523e-4578-98fb-9e77db3bcb48"
      },
      "source": [
        "x, y = get_batch(vectorized_windows, seq_length=100, batch_size=32)\n",
        "pred = model(x)\n",
        "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
        "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape:       (32, 100)  # (batch_size, sequence_length)\n",
            "Prediction shape:  (32, 100, 12) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT1HvFVUGpoE",
        "colab_type": "text"
      },
      "source": [
        "### **4.4.3 Predictions from the untrained model\n",
        "\n",
        "Let's take a look at what our untrained model is predicting.\n",
        "\n",
        "To get actual predictions from the model, we sample from the output distribution, which is defined by a `softmax` over our character vocabulary. This will give us actual character indices. This means we are using a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) to sample over the example prediction. This gives a prediction of the next character (specifically its index) at each timestep.\n",
        "\n",
        "Note here that we sample from this probability distribution, as opposed to simply taking the `argmax`, which can cause the model to get stuck in a loop.\n",
        "\n",
        "Let's try this sampling out for the first example in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4V4MfFg0RQJg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "aa71e7b3-ee0a-4870-fb21-d966e09e9e6b"
      },
      "source": [
        "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11,  4,  2, 10,  1,  1,  4,  4,  5,  3,  4,  3,  6,  0,  1,  4, 11,\n",
              "        9,  5,  0,  9,  2,  2, 11,  9,  6,  7,  6,  7,  0, 11,  5, 10, 10,\n",
              "        5,  9,  5, 10,  8,  2,  2,  5, 10,  8,  4,  2,  8,  0,  8,  8,  3,\n",
              "        3,  0,  1, 10,  3,  2,  2,  0,  9,  5,  5,  6, 11,  0,  3,  4,  6,\n",
              "        8, 11,  5, 10, 10,  8,  0,  3,  1,  9,  5,  9,  0,  1,  5,  0,  9,\n",
              "        3,  6,  4,  3,  4,  3,  9,  3,  1, 11,  7,  2,  7,  1,  9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "We can now decode these to see the text predicted by the untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "bc58978a-ab81-4d5e-a1e0-4bb77eed336c"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " '36\\n0.321\\n0.393\\n0.357\\n0.393\\n0.179\\n0.321\\n0.464\\n0.607\\n0.393\\n0.536\\n0.357\\n0.214\\n\\n0.179\\n0.179\\n0.107\\n0.250\\n'\n",
            "\n",
            "Next Char Predictions: \n",
            " '9208..2231214\\n.2973\\n700974545\\n93883738600386206\\n6611\\n.8100\\n73349\\n124693886\\n1.737\\n.3\\n714212171.9505.7'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEHHcRasIDm9",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the text predicted by the untrained model is pretty nonsensical! How can we do better? We can train the network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## 4.5 Training the model: loss and training operations\n",
        "\n",
        "At this point, we can think of our next character prediction problem as a standard classification problem. Given the previous state of the RNN, as well as the input at a given time step, we want to predict the class of the next character. \n",
        "\n",
        "To train our model on this classification task, we can use a form of the `crossentropy` loss (negative log likelihood loss). Specifically, we will use the [`sparse_categorical_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/sparse_categorical_crossentropy) loss, as it utilizes integer targets for categorical classification tasks. We will want to compute the loss using the true targets -- the `labels` -- and the predicted targets -- the `logits`.\n",
        "\n",
        "Let's first compute the loss using our example predictions from the untrained model, i.e. the loss from a random target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3ded741e-e92c-4a29-b8f9-cca8a35e2f75"
      },
      "source": [
        "### Defining the loss function ###\n",
        "\n",
        "'''define the loss function to compute and return the loss between\n",
        "    the true labels and predictions (logits). Set the argument from_logits=True.'''\n",
        "def compute_loss(labels, logits):\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "  \n",
        "  return loss\n",
        "\n",
        "'''compute the loss using the true next characters from the example batch \n",
        "    and the predictions from the untrained model several cells above'''\n",
        "example_batch_loss = compute_loss(y, pred)\n",
        "\n",
        "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (32, 100, 12)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       2.4853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Seh7e6eRqd7",
        "colab_type": "text"
      },
      "source": [
        "Let's start by defining some hyperparameters for training the model. Given our hardware some reasonable values for some of the parameters are given below as ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQWUUhKotkAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Hyperparameter setting and optimization ###\n",
        "\n",
        "# Optimization parameters:\n",
        "num_training_iterations = 1000  # Increase this to train longer\n",
        "batch_size = 128  # Experiment between 1 and 64\n",
        "seq_length = 256  # Experiment between 50 and 500\n",
        "learning_rate = 1e-4  # Experiment between 1e-5 and 1e-1\n",
        "\n",
        "# Model parameters: \n",
        "vocabulary_size = len(vocabulary)\n",
        "embedding_dim = 256 \n",
        "rnn_units = 2048  # Experiment between 1 and 2048\n",
        "\n",
        "# Checkpoint location and callback function: \n",
        "checkpoint_dir = os.path.join(path,'./training_checkpoints')\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMYNf0h40Yx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ee9bd99e-0000-44c4-98cb-64a173ce8e5c"
      },
      "source": [
        "checkpoint_dir"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/crime_prediction/./training_checkpoints'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cu11p1MKYZd",
        "colab_type": "text"
      },
      "source": [
        "Now, we are ready to define our training operation -- the optimizer and duration of training -- and use this function to train the model. You will experiment with the choice of optimizer and the duration for which you train your models, and see how these changes affect the network's output. Some optimizers you may like to try are [`Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam?version=stable) and [`Adagrad`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad?version=stable).\n",
        "\n",
        "First, we will instantiate a new model and an optimizer. Then, we will use the [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) method to perform the backpropagation operations. \n",
        "\n",
        "We will also generate a print-out of the model's progress through training, which will help us easily visualize whether or not we are minimizing the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F31vzJ_u66cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "2d7818ba-0305-4635-b9ee-9922118904fd"
      },
      "source": [
        "### Define optimizer and training operation ###\n",
        "\n",
        "'''instantiate a new model for training using the `build_model`\n",
        "  function and the hyperparameters created above.'''\n",
        "model = build_model(vocabulary_size, embedding_dim, rnn_units, batch_size)\n",
        "# model = build_model('''TODO: arguments''')\n",
        "\n",
        "'''instantiate an optimizer with its learning rate.\n",
        "  Checkout the tensorflow website for a list of supported optimizers.\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/\n",
        "  Try using the Adam optimizer to start.'''\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "# optimizer = # TODO\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y): \n",
        "  # Use tf.GradientTape()\n",
        "  with tf.GradientTape() as tape:\n",
        "  \n",
        "    '''feed the current input into the model and generate predictions'''\n",
        "    y_hat = model(x) # TODO\n",
        "    # y_hat = model('''TODO''')\n",
        "  \n",
        "    '''compute the loss!'''\n",
        "    loss = compute_loss(y, y_hat) # TODO\n",
        "    # loss = compute_loss('''TODO''', '''TODO''')\n",
        "    '''compute the accuracy!'''\n",
        "    #accuracy=keras.metrics.accuracy(y, y_hat)\n",
        "\n",
        "  # Now, compute the gradients \n",
        "  '''complete the function call for gradient computation. \n",
        "      Remember that we want the gradient of the loss with respect all \n",
        "      of the model parameters. \n",
        "      HINT: use `model.trainable_variables` to get a list of all model\n",
        "      parameters.'''\n",
        "  grads = tape.gradient(loss, model.trainable_variables) # TODO\n",
        "  # grads = tape.gradient('''TODO''', '''TODO''')\n",
        "  \n",
        "  # Apply the gradients to the optimizer so it can update the model accordingly\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "##################\n",
        "# Begin training!#\n",
        "##################\n",
        "\n",
        "history = []\n",
        "plotter = util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "for iter in tqdm(range(num_training_iterations)):\n",
        "\n",
        "  # Grab a batch and propagate it through the network\n",
        "  x_batch, y_batch = get_batch(vectorized_windows, seq_length, batch_size)\n",
        "  loss = train_step(x_batch, y_batch)\n",
        "\n",
        "  # Update the progress bar\n",
        "  history.append(loss.numpy().mean())\n",
        "  plotter.plot(history)\n",
        "\n",
        "  # Update the model with the changed weights!\n",
        "  if iter % 100 == 0:     \n",
        "    model.save_weights(checkpoint_prefix)\n",
        "    \n",
        "# Save the trained model and the weights\n",
        "model.save_weights(checkpoint_prefix)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|█▉        | 198/1000 [08:03<32:16,  2.41s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## 4.6 Generate crime forecasted sequence\n",
        "\n",
        "When generating future crime sequences, we'll have to feed the model some sort of seed to get it started (because it can't predict anything without something to start with!).\n",
        "\n",
        "Once we have a generated seed, we can then iteratively predict each successive character using our trained RNN. \n",
        "\n",
        "More specifically, recall that our RNN outputs a `softmax` over possible successive characters. For inference, we iteratively sample from these distributions, and then use our samples to encode a generated crime sequence in a time series format or whatever the format of your training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### 4.6.1 Restore the latest checkpoint\n",
        "\n",
        "To keep this inference step simple, we will use a batch size of 1. Because of how the RNN state is passed from timestep to timestep, the model will only be able to accept a fixed batch size once it is built. \n",
        "\n",
        "To run the model with a different `batch_size`, we'll need to rebuild the model and restore the weights from the latest checkpoint, i.e., the weights after the last checkpoint during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "colab": {}
      },
      "source": [
        "'''Rebuild the model using a batch_size=1'''\n",
        "model = build_model(vocabulary_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "# Restore the model weights for the last checkpoint after training\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9b4V2C8N62l",
        "colab_type": "text"
      },
      "source": [
        "Notice that we have fed in a fixed `batch_size` of 1 for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### 4.6.2 The prediction procedure or loop\n",
        "\n",
        "Now, we're ready to write the code to generate predictions:\n",
        "\n",
        "* Initialize a \"seed\" start string and the RNN state, and set the number of characters we want to generate.\n",
        "\n",
        "* Use the start string and the RNN state to obtain the probability distribution over the next predicted character.\n",
        "\n",
        "* Sample from multinomial distribution to calculate the index of the predicted character. This predicted character is then used as the next input to the model.\n",
        "\n",
        "* At each time step, the updated RNN state is fed back into the model, so that it now has more context in making the next prediction. After predicting the next character, the updated RNN states are again fed back into the model, which is how it learns sequence dependencies in the data, as it gets more information from the previous predictions.\n",
        "\n",
        "![LSTM inference](https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab1/img/lstm_inference.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string, generation_length=1000):\n",
        "  # Evaluation step (generating forecast using the trained RNN model)\n",
        "\n",
        "  '''convert the start string to numbers (vectorize)'''\n",
        "  input_eval = [char2idx[s] for s in start_string] \n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  tqdm._instances.clear()\n",
        "\n",
        "  for i in tqdm(range(generation_length)):\n",
        "      #evaluate the inputs and generate the next character predictions\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # Remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      \n",
        "      #Use a multinomial distribution to sample\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Pass the prediction along with the previous hidden state\n",
        "      #   as the next inputs to the model\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      \n",
        "      #Add the predicted character to the generated text\n",
        "      #consider what format the prediction is in vs. the output\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "    \n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktovv0RFhrkn",
        "colab": {}
      },
      "source": [
        "'''Use the model and the function defined above to generate a sequence in the input\n",
        " format text of length 1000!\n",
        "'''\n",
        "try:\n",
        "  generated_text = generate_text(model, start_string=\"\\n\", generation_length=100000) \n",
        "except:\n",
        "  print('Some error ocurred')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "### 4.6.3 Save predicted crime sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFJE55ZjG8es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v42KcPq-0OpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_to_path=os.path.join(path,'data','prediction','rnn_output.txt')\n",
        "with open(save_to_path,mode='w') as file:\n",
        "  file.write(generated_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NYNdXs5Gi8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save hyperparameters config\n",
        "num_training_iterations = 1000  # Increase this to train longer\n",
        "batch_size = 128  # Experiment between 1 and 64\n",
        "seq_length = 256  # Experiment between 50 and 500\n",
        "learning_rate = 1e-4  # Experiment between 1e-5 and 1e-1\n",
        "# Model parameters: \n",
        "vocabulary_size = len(vocabulary)\n",
        "embedding_dim = 256 \n",
        "rnn_units = 2048  # Experiment between 1 and 2048\n",
        "\n",
        "save_to_path=os.path.join(path,'data','prediction','rnn_output-'+str(batch_size)+'-'+str(seq_length)+'-'+str(str_units)+'.txt')\n",
        "with open(save_to_path,mode='w') as file:\n",
        "  file.write(generated_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx3rsk550KIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Avoid disconnect\n",
        "import time\n",
        "\n",
        "i=0\n",
        "while True:\n",
        "  time.sleep(60) \n",
        "  i+=1\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_ftOppH0qud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}